{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is superceded by `tracker_analysis.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import utils\n",
    "import csv\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV files for analysis\n",
    "Note: Running this cell block will append lines to existing CSV files. Delete existing CSV files before each new run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"analysis\"):\n",
    "    os.mkdir(\"analysis\")\n",
    "\n",
    "\n",
    "def detect_tracking(blocklist, url_list):\n",
    "    \"\"\"\n",
    "    Check if any URLs from a list appear in a blocklist of known tracking cookies.\n",
    "\n",
    "    Args:\n",
    "        blocklist: Set of blocked domains.\n",
    "        url_list: List of URLs.\n",
    "\n",
    "    Returns:\n",
    "        A list of detected trackers.\n",
    "    \"\"\"\n",
    "\n",
    "    detected_trackers = []\n",
    "    for url in url_list:\n",
    "        if utils.get_full_domain(url) in blocklist: # FIXME: check if this is the correct way to get domain\n",
    "            detected_trackers.append(url)\n",
    "\n",
    "    return detected_trackers\n",
    "\n",
    "\n",
    "def get_urls_from_har(file: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of cookies from an HAR file.\n",
    "    [HAR Specification](http://www.softwareishard.com/blog/har-12-spec/).\n",
    "\n",
    "    Args:\n",
    "        file: Path to the HAR file.\n",
    "    Returns:\n",
    "        A list of cookies.\n",
    "    \"\"\"\n",
    "\n",
    "    all_urls = []\n",
    "    data = json.load(open(file, \"r\")) # parses JSON data into Python dictionary\n",
    "    for entry in data[\"log\"][\"entries\"]: # each entry is an HTTP request/response pair\n",
    "        request = entry[\"request\"] # extract request dictionary\n",
    "\n",
    "        if (url := request.get(\"url\")) and request.get(\"cookies\"): # valid URL exists and request contains cookies\n",
    "            all_urls.append(url)\n",
    "\n",
    "    return all_urls\n",
    "\n",
    "\n",
    "def get_tracking_sites(list_path: str = \"inputs/blocklists/\") -> set[str]:\n",
    "    \"\"\"\n",
    "    Get tracking sites from blocklists.\n",
    "\n",
    "    Args:\n",
    "        list_path: Path to blocklists. Defaults to \"inputs/blocklists/\".\n",
    "\n",
    "    Returns:\n",
    "        A set of tracking sites.\n",
    "    \"\"\"\n",
    "    lists = []\n",
    "    for item in os.listdir(list_path):\n",
    "        path = os.path.join(list_path, item)\n",
    "        lists.append(path)\n",
    "\n",
    "    tracking_sites = set()\n",
    "    for list_path in lists:\n",
    "        with open(list_path) as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                tracking_sites.add(line.rstrip())\n",
    "\n",
    "    # print(\"Tracking sites aggregated from 4 blocklists.\")\n",
    "    return tracking_sites\n",
    "\n",
    "\n",
    "def get_directories(root: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Return a list of directories in a given root directory.\n",
    "\n",
    "    Args:\n",
    "        root: Path to the root directory.\n",
    "\n",
    "    Returns:\n",
    "        A list of directories.\n",
    "    \"\"\"\n",
    "    dirs = []\n",
    "    for item in os.listdir(root):\n",
    "        path = os.path.join(root, item)\n",
    "        if os.path.isdir(path):\n",
    "            dirs.append(path)\n",
    "\n",
    "    return dirs\n",
    "\n",
    "\n",
    "# Create set of tracking sites from aggregation of 4 blocklists\n",
    "trackings_sites = get_tracking_sites()\n",
    "# print(trackings_sites)\n",
    "\n",
    "\n",
    "def analyze_har(har_path: str):\n",
    "    \"\"\"\n",
    "    Return a list of tracking cookies detected in the specified HAR file.\n",
    "\n",
    "    Args:\n",
    "        har_path: Path to the HAR file.\n",
    "\n",
    "    Returns:\n",
    "        A list of detected tracking cookies.\n",
    "    \"\"\"\n",
    "    urls = get_urls_from_har(har_path) # get list of URLs\n",
    "    detected_list = detect_tracking(trackings_sites, urls)\n",
    "    return detected_list\n",
    "\n",
    "\n",
    "success_file_path = \"inputs/sites/success.txt\"\n",
    "with open(success_file_path, \"r\") as success_file:\n",
    "    success_lines = success_file.readlines()\n",
    "\n",
    "# TODO: change filepath\n",
    "# domain_paths = get_directories(\"crawls/depth1_noquery\") \n",
    "domain_paths = get_directories(\"crawls/depth0\") \n",
    "\n",
    "# Initialize dictionaries to store inner site paths for normal and after_reject crawls\n",
    "domains_paths_normal = {}\n",
    "domains_paths_reject = {}\n",
    "\n",
    "incomplete_runs = 0\n",
    "total_inner_pages = 0\n",
    "\n",
    "detected_trackers_normal = []\n",
    "detected_trackers_after_reject = []\n",
    "\n",
    "for site in domain_paths:\n",
    "    # Skip if site is not in success.txt\n",
    "    # FIXME: success.txt currently not formatted properly; uncommenting this causes no rows to be written to CSV\n",
    "    # if not any(site in line for line in success_lines):\n",
    "    #     continue\n",
    "\n",
    "    inner_site_paths = get_directories(site)\n",
    "    total_inner_pages += len(inner_site_paths)\n",
    "\n",
    "    for inner_site_path in inner_site_paths:\n",
    "        normal_har_path = f\"{inner_site_path}/normal.json\"\n",
    "        reject_har_path = f\"{inner_site_path}/after_reject.json\"\n",
    "\n",
    "        if not os.path.isfile(normal_har_path) or not os.path.isfile(reject_har_path):\n",
    "            # Requires both normal and intercept HAR files to exist\n",
    "            incomplete_runs += 1\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        domain = site.split(\"/\")[2]\n",
    "        \n",
    "        # Append inner site path to the dictionary for normal crawls\n",
    "        if domain in domains_paths_normal:\n",
    "            domains_paths_normal[domain].append(inner_site_path)\n",
    "        else:\n",
    "            domains_paths_normal[domain] = [inner_site_path]\n",
    "\n",
    "        # Append inner site path to the dictionary for after_reject crawls\n",
    "        if domain in domains_paths_reject:\n",
    "            domains_paths_reject[domain].append(inner_site_path)\n",
    "        else:\n",
    "            domains_paths_reject[domain] = [inner_site_path]\n",
    "\n",
    "        detected_list_normal = analyze_har(normal_har_path)\n",
    "\n",
    "\n",
    "        for tracker_url in detected_list_normal:\n",
    "            detected_trackers_normal.append({\n",
    "                \"Domain\": site.split(\"/\")[2],\n",
    "                \"Inner Site Path\": inner_site_path,\n",
    "                \"Tracker\": tracker_url\n",
    "            })\n",
    "\n",
    "        # Create file if it doesn't exist; if it exists then write a row for each inner site path with a count of the number of trackers.\n",
    "        # TODO: change name\n",
    "        # normal_file = \"analysis/depth1_noquery_normal.csv\"\n",
    "        normal_file = \"analysis/depth0_normal.csv\"\n",
    "        normal_file_exists = os.path.isfile(normal_file)\n",
    "\n",
    "        if normal_file_exists:\n",
    "            with open(normal_file, mode=\"a\", newline=\"\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([inner_site_path, len(detected_list_normal)])\n",
    "                file.flush() # bugfix where rows weren't writing: flush() clears internal buffer\n",
    "\n",
    "        else:\n",
    "            with open(normal_file, mode=\"w\", newline=\"\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\"Inner Site Path\", \"Length of Detected List\"])\n",
    "                writer.writerow([inner_site_path, len(detected_list_normal)])\n",
    "                file.flush()\n",
    "\n",
    "\n",
    "        # Repeat for files generated after run with intercept.\n",
    "        detected_list_reject = analyze_har(reject_har_path)\n",
    "\n",
    "        for tracker_url in detected_list_reject:\n",
    "            detected_trackers_after_reject.append({\n",
    "                \"Domain\": site.split(\"/\")[2],\n",
    "                \"Inner Site Path\": inner_site_path,\n",
    "                \"Tracker\": tracker_url\n",
    "            })\n",
    "\n",
    "        # TODO: change name\n",
    "        # reject_file = \"analysis/depth1_noquery_after_reject.csv\"\n",
    "        reject_file = \"analysis/depth0_after_reject.csv\"\n",
    "        reject_file_exists = os.path.isfile(reject_file)\n",
    "\n",
    "        if reject_file_exists:\n",
    "            with open(reject_file, mode=\"a\", newline=\"\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([inner_site_path, len(detected_list_reject)])\n",
    "                file.flush()\n",
    "        else:\n",
    "            with open(reject_file, mode=\"w\", newline=\"\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\"Inner Site Path\", \"Length of Detected List\"])\n",
    "                writer.writerow([inner_site_path, len(detected_list_reject)])\n",
    "                file.flush()\n",
    "\n",
    "# Create DataFrames for detected trackers in normal and after_reject crawls\n",
    "# Each tracker is in a row with its domain and inner site path\n",
    "df_normal = pd.DataFrame(detected_trackers_normal)\n",
    "df_after_reject = pd.DataFrame(detected_trackers_after_reject)\n",
    "\n",
    "df_normal.info()\n",
    "df_after_reject.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_normal.tail(15)\n",
    "# df_after_reject.tail(15)\n",
    "\n",
    "# Check if the entire DataFrame contains null values\n",
    "if df_normal.isnull().any().any():\n",
    "    print(\"df_normal contains null values.\")\n",
    "\n",
    "if df_after_reject.isnull().any().any():\n",
    "    print(\"df_after_reject contains null values.\")\n",
    "\n",
    "if df_normal['Tracker'].isnull().any():\n",
    "    print(\"Tracker col contains null values.\")\n",
    "\n",
    "\n",
    "# df_normal_stackoverflow = df_normal.loc[df_normal['Domain'] == 'stackoverflow.com']\n",
    "# print(df_normal_stackoverflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by domain and set Num_Inner_Pages using the dictionary domains_paths_normal\n",
    "df_normal_domains = df_normal.groupby('Domain', as_index=False).agg(\n",
    "    Num_Inner_Pages=('Domain', lambda x: len(domains_paths_normal.get(x.iloc[0], []))), # Use the length of inner site paths in the domains_paths_normal dictionary\n",
    "    Num_Trackers_Per_Domain=('Tracker', 'count')  # Count the number of trackers for each domain\n",
    ")\n",
    "\n",
    "df_normal_domains[\"Average Trackers Per Page\"] = df_normal_domains[\"Num_Trackers_Per_Domain\"] / df_normal_domains[\"Num_Inner_Pages\"]\n",
    "\n",
    "\n",
    "# Group by domain and set Num_Inner_Pages using the dictionary domains_paths_reject\n",
    "df_after_reject_domains = df_after_reject.groupby('Domain', as_index=False).agg(\n",
    "    Num_Inner_Pages=('Domain', lambda x: len(domains_paths_reject.get(x.iloc[0], []))), # Use the length of inner site paths in the domains_paths_reject dictionary\n",
    "    Num_Trackers_Per_Domain=('Tracker', 'count')  # Count the number of trackers for each domain\n",
    ")\n",
    "\n",
    "df_after_reject_domains[\"Average Trackers Per Page\"] = df_after_reject_domains[\"Num_Trackers_Per_Domain\"] / df_after_reject_domains[\"Num_Inner_Pages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_normal.info()\n",
    "# df_normal_domains.info()\n",
    "df_normal_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_after_reject_domains.info()\n",
    "df_after_reject_domains\n",
    "# df_after_reject.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Tracking Cookies Across All Inner Pages (Regardless of Domain)\n",
    "Run this cell to check that number of complete+incomplete pages equals total inner pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_trackers(reject_filepath, normal_filepath):\n",
    "    no_trackers_after_reject = []  # List of inner site paths with trackers in normal crawl, but no trackers after rejection\n",
    "    increased_trackers = []  # List of inner site paths with more trackers after rejection than in normal crawl\n",
    "    never_trackers = []  # List of inner site paths with no trackers in either normal or rejection crawl\n",
    "    violating_sites = []  # List of inner site paths with trackers after we click the reject button\n",
    "\n",
    "    with open(reject_filepath, 'r') as reject_file, open(normal_filepath, 'r') as normal_file:\n",
    "        read_reject = csv.reader(reject_file)\n",
    "        read_normal = csv.reader(normal_file)\n",
    "\n",
    "        # Skip header\n",
    "        next(read_reject)\n",
    "        next(read_normal)\n",
    "\n",
    "        length = 0\n",
    "\n",
    "        # Since both csvs are sorted by inner site path, we can just iterate through both at the same time\n",
    "        for normal, after_reject in zip(read_normal, read_reject):\n",
    "            inner_site_path, num_trackers_normal = normal\n",
    "            _, num_trackers_reject = after_reject\n",
    "\n",
    "            if inner_site_path != _:\n",
    "                raise RuntimeError(\"Inner site paths do not match\")\n",
    "\n",
    "            num_trackers_normal = int(num_trackers_normal)\n",
    "            num_trackers_reject = int(num_trackers_reject)\n",
    "\n",
    "            if num_trackers_normal > 0 and num_trackers_reject == 0:  # if there are trackers in normal crawl, but not after reject\n",
    "                no_trackers_after_reject.append(inner_site_path)\n",
    "\n",
    "            if num_trackers_normal < num_trackers_reject:  # if there are more trackers after reject than in normal crawl\n",
    "                increased_trackers.append(inner_site_path)\n",
    "\n",
    "            if num_trackers_normal == 0 and num_trackers_reject == 0:  # if there are no trackers in either normal or reject\n",
    "                never_trackers.append(inner_site_path)\n",
    "\n",
    "            if num_trackers_reject != 0:  # if there are trackers in reject\n",
    "                violating_sites.append(inner_site_path)\n",
    "\n",
    "            length += 1\n",
    "\n",
    "    # from previous cell\n",
    "    print(\"Total inner pages:\", total_inner_pages)\n",
    "    print(\"Incomplete inner pages:\", incomplete_runs)\n",
    "    \n",
    "    print(\"Complete inner pages:\", length)\n",
    "    print(\"Inner pages that removed all trackers after rejection:\", len(no_trackers_after_reject))\n",
    "    print(\"Inner pages with increased trackers after rejection:\", len(increased_trackers))\n",
    "    print(\"Inner pages that never contained trackers:\", len(never_trackers))\n",
    "    print(\"Inner pages that sent cookies to 3rd party trackers after rejection:\", len(violating_sites))\n",
    "\n",
    "\n",
    "def get_length_detected_list(csv_reader, inner_site_path):\n",
    "    for row in csv_reader:\n",
    "        current_inner_site_path, length_detected_list = row\n",
    "        if current_inner_site_path == inner_site_path:\n",
    "            return length_detected_list\n",
    "\n",
    "    return '0'  # If inner_site_path not found, return '0'\n",
    "\n",
    "\n",
    "compare_trackers('analysis/depth0_after_reject.csv', 'analysis/depth0_normal.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cookie-classify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
