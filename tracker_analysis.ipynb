{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook analyzes crawls using the `compliance_algo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import utils\n",
    "import csv\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Analysis and Creating Blocklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"analysis\"):\n",
    "    os.mkdir(\"analysis\")\n",
    "\n",
    "def get_tracking_domains(list_path: str = \"inputs/blocklists/\") -> set[str]:\n",
    "    \"\"\"\n",
    "    Get tracking domains from blocklists.\n",
    "\n",
    "    Args:\n",
    "        list_path: Path to blocklists. Defaults to \"inputs/blocklists/\".\n",
    "\n",
    "    Returns:\n",
    "        A set of tracking domains.\n",
    "    \"\"\"\n",
    "    lists = []\n",
    "    for item in os.listdir(list_path):\n",
    "        path = os.path.join(list_path, item)\n",
    "        lists.append(path)\n",
    "\n",
    "    tracking_sites = set()\n",
    "    for list_path in lists:\n",
    "        with open(list_path) as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                tracking_sites.add(line.rstrip())\n",
    "\n",
    "    # print(\"Tracking sites aggregated from 4 blocklists.\")\n",
    "    return tracking_sites\n",
    "\n",
    "# Create set of tracking domains from aggregation of 4 blocklists\n",
    "trackings_domains = get_tracking_domains()\n",
    "print(trackings_domains)\n",
    "\n",
    "def get_directories(root: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Return a list of directories in a given root directory.\n",
    "\n",
    "    Args:\n",
    "        root: Path to the root directory.\n",
    "\n",
    "    Returns:\n",
    "        A list of directories.\n",
    "    \"\"\"\n",
    "    dirs = []\n",
    "    for item in os.listdir(root):\n",
    "        path = os.path.join(root, item)\n",
    "        if os.path.isdir(path):\n",
    "            dirs.append(path)\n",
    "\n",
    "    return dirs\n",
    "\n",
    "\n",
    "def detect_tracking(blocklist, cookie_list) -> list[dict[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Check if any URLs from a list appear in a blocklist of known tracking cookies.\n",
    "\n",
    "    Args:\n",
    "        blocklist: Set of blocked domains.\n",
    "        cookie_list: List of cookies, where each cookie is a dict of 3 key-value pairs.\n",
    "\n",
    "    Returns:\n",
    "        A filtered list of detected tracking cookies.\n",
    "    \"\"\"\n",
    "\n",
    "    detected_trackers = []\n",
    "    for cookie in cookie_list:\n",
    "        cookie_domain = cookie[\"Cookie Domain\"]\n",
    "        if utils.get_domain(cookie_domain) in blocklist:\n",
    "            detected_trackers.append(cookie)\n",
    "\n",
    "    return detected_trackers\n",
    "\n",
    "\n",
    "def get_cookies_from_har(file: str) -> list[dict[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Returns a list of cookies from response entries in an HAR file.\n",
    "    [HAR Specification](http://www.softwareishard.com/blog/har-12-spec/).\n",
    "\n",
    "    Args:\n",
    "        file: Path to the HAR file.\n",
    "    Returns:\n",
    "        A list of dictionaries representing all cookies in HTTP responses in that HAR file with domains, where each dictionary holds 3 key-value pairs (Cookie Name, Cookie Value, Cookie Domain).\n",
    "    \"\"\"\n",
    "\n",
    "    cookies = []\n",
    "    data = json.load(open(file, \"r\")) # parses JSON data into Python dictionary\n",
    "    for entry in data[\"log\"][\"entries\"]: # each entry is an HTTP request/response pair\n",
    "        \n",
    "        response = entry[\"response\"] # extract response dictionary\n",
    "\n",
    "        if response.get(\"cookies\"): # response contains cookies\n",
    "            for cookie in response[\"cookies\"]:\n",
    "                # print(cookie)\n",
    "                if cookie.get(\"domain\"): # if cookie has domain\n",
    "                    cookies.append({\"Cookie Name\": cookie[\"name\"], \"Cookie Value\": cookie[\"value\"], \"Cookie Domain\": cookie[\"domain\"]})\n",
    "\n",
    "    return cookies\n",
    "\n",
    "def check_requests(detected_list_from_responses: list[dict[str, str, str]], file: str) -> list[dict[str, str, str]]:\n",
    "    \n",
    "    detected_list_from_requests = []\n",
    "    data = json.load(open(file, \"r\")) # parses JSON data into Python dictionary\n",
    "    for entry in data[\"log\"][\"entries\"]: # each entry is an HTTP request/response pair\n",
    "        \n",
    "        request = entry[\"request\"] # extract request dictionary\n",
    "\n",
    "        for cookie in request.get(\"cookies\"):\n",
    "            values_of_cookie_names = [d[\"Cookie Name\"] for d in detected_list_from_responses]\n",
    "            if cookie.get(\"name\") in values_of_cookie_names: # if cookie name is in list of detected cookies from responses\n",
    "                detected_list_from_requests.append({\"Cookie Name\": cookie[\"name\"], \"Cookie Value\": cookie[\"value\"]})\n",
    "\n",
    "    return detected_list_from_requests\n",
    "\n",
    "def analyze_har(har_path: str) -> list[dict[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Return a list of tracking cookies detected in the requests of a specified HAR file.\n",
    "\n",
    "    Args:\n",
    "        har_path: Path to the HAR file.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries representing detected tracking cookies from requests, where each dictionary holds 3 key-value pairs (Cookie Name, Cookie Value, Cookie Domain).\n",
    "    \"\"\"\n",
    "    cookies = get_cookies_from_har(har_path)\n",
    "    filtered_list = detect_tracking(trackings_domains, cookies)\n",
    "    return filtered_list\n",
    "\n",
    "# print(get_cookies_from_har(\"crawls/depth0/bmj.com/0/normal.json\"))\n",
    "# print(analyze_har(\"crawls/depth0/bmj.com/0/normal.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframes and Generate CSV Files\n",
    "Note: Running this cell block will append lines to existing CSV files. Delete existing CSV files or comment out lines before each new run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain_paths = get_directories(\"crawls/depth1\") \n",
    "domain_paths = get_directories(\"crawls/depth0\") \n",
    "\n",
    "# for counting number of inner pages per domain\n",
    "domains_paths_normal = {}\n",
    "domains_paths_reject = {}\n",
    "\n",
    "incomplete_runs = 0\n",
    "total_inner_pages = 0\n",
    "\n",
    "detected_trackers_from_responses_normal = []\n",
    "detected_trackers_from_requests_normal = [] # will be used to create DataFrame\n",
    "\n",
    "detected_trackers_from_responses_reject = []\n",
    "detected_trackers_from_requests_reject = [] # will be used to create Dataframe\n",
    "\n",
    "for site in domain_paths:\n",
    "    # Skip if site is not in success.txt\n",
    "    # FIXME: success.txt currently not formatted properly; uncommenting this causes no rows to be written to CSV\n",
    "    # if not any(site in line for line in success_lines):\n",
    "    #     continue\n",
    "\n",
    "    inner_site_paths = get_directories(site)\n",
    "    total_inner_pages += len(inner_site_paths)\n",
    "\n",
    "    for inner_site_path in inner_site_paths:\n",
    "        normal_har_path = f\"{inner_site_path}/normal.json\"\n",
    "        reject_har_path = f\"{inner_site_path}/after_reject.json\"\n",
    "\n",
    "        if not os.path.isfile(normal_har_path) or not os.path.isfile(reject_har_path):\n",
    "            # Requires both normal and intercept HAR files to exist\n",
    "            incomplete_runs += 1\n",
    "            continue\n",
    "            \n",
    "        domain = site.split(\"/\")[2]\n",
    "\n",
    "        # Append inner site path to the dictionary for normal crawls\n",
    "        if domain in domains_paths_normal:\n",
    "            domains_paths_normal[domain].append(inner_site_path)\n",
    "        else:\n",
    "            domains_paths_normal[domain] = [inner_site_path]\n",
    "\n",
    "        # Append inner site path to the dictionary for after_reject crawls\n",
    "        if domain in domains_paths_reject:\n",
    "            domains_paths_reject[domain].append(inner_site_path)\n",
    "        else:\n",
    "            domains_paths_reject[domain] = [inner_site_path]\n",
    "\n",
    "        detected_list_from_requests_normal = analyze_har(normal_har_path)\n",
    "\n",
    "        # saving trackers from responses for easy parsing into dataframe if needed\n",
    "        for detected_tracker in detected_list_from_requests_normal:\n",
    "            detected_trackers_from_responses_normal.append({\n",
    "                \"Domain\": domain,\n",
    "                \"Inner Site Path\": inner_site_path,\n",
    "                \"Cookie Name\": detected_tracker[\"Cookie Name\"],\n",
    "                \"Cookie Value\": detected_tracker[\"Cookie Value\"],\n",
    "                \"Cookie Domain\": detected_tracker[\"Cookie Domain\"]\n",
    "            })\n",
    "\n",
    "        trackers_requests_normal = check_requests(detected_trackers_from_responses_normal, normal_har_path)\n",
    "        \n",
    "        for detected_tracker in trackers_requests_normal:\n",
    "            detected_trackers_from_requests_normal.append({\n",
    "                \"Domain\": domain,\n",
    "                \"Inner Site Path\": inner_site_path,\n",
    "                \"Cookie Name\": detected_tracker[\"Cookie Name\"],\n",
    "                \"Cookie Value\": detected_tracker[\"Cookie Value\"],\n",
    "                # TODO: maybe we can use reqeust.url to get the domain?\n",
    "            })\n",
    "\n",
    "        # # Create file if it doesn't exist; if it exists then write a row for each inner site path with a count of the number of trackers.\n",
    "        # normal_file = \"analysis/depth1_normal.csv\"\n",
    "        # normal_file_exists = os.path.isfile(normal_file)\n",
    "\n",
    "        # if normal_file_exists:\n",
    "        #     with open(normal_file, mode=\"a\", newline=\"\") as file:\n",
    "        #         writer = csv.writer(file)\n",
    "        #         writer.writerow([inner_site_path, len(trackers_requests_normal)])\n",
    "        #         file.flush() # bugfix where rows weren't writing: flush() clears internal buffer\n",
    "\n",
    "        # else:\n",
    "        #     with open(normal_file, mode=\"w\", newline=\"\") as file:\n",
    "        #         writer = csv.writer(file)\n",
    "        #         writer.writerow([\"Inner Site Path\", \"Length of Detected List\"])\n",
    "        #         writer.writerow([inner_site_path, len(trackers_requests_normal)])\n",
    "        #         file.flush()\n",
    "\n",
    "\n",
    "        # Repeat for files generated after run with rejecting cookies\n",
    "        detected_list_from_requests_reject = analyze_har(reject_har_path)\n",
    "\n",
    "        # saving trackers from responses for easy parsing into dataframe if needed\n",
    "        for detected_tracker in detected_list_from_requests_reject:\n",
    "            detected_trackers_from_responses_reject.append({\n",
    "                \"Domain\": domain,\n",
    "                \"Inner Site Path\": inner_site_path,\n",
    "                \"Cookie Name\": detected_tracker[\"Cookie Name\"],\n",
    "                \"Cookie Value\": detected_tracker[\"Cookie Value\"],\n",
    "                \"Cookie Domain\": detected_tracker[\"Cookie Domain\"]\n",
    "            })\n",
    "\n",
    "        trackers_requests_reject = check_requests(detected_trackers_from_responses_reject, reject_har_path)\n",
    "        \n",
    "        for detected_tracker in trackers_requests_reject:\n",
    "            detected_trackers_from_requests_reject.append({\n",
    "                \"Domain\": domain,\n",
    "                \"Inner Site Path\": inner_site_path,\n",
    "                \"Cookie Name\": detected_tracker[\"Cookie Name\"],\n",
    "                \"Cookie Value\": detected_tracker[\"Cookie Value\"],\n",
    "            })\n",
    "\n",
    "        # # Create file if it doesn't exist; if it exists then write a row for each inner site path with a count of the number of trackers.\n",
    "        # reject_file = \"analysis/depth1_after_reject.csv\"\n",
    "        # reject_file_exists = os.path.isfile(reject_file)\n",
    "\n",
    "        # if reject_file_exists:\n",
    "        #     with open(reject_file, mode=\"a\", newline=\"\") as file:\n",
    "        #         writer = csv.writer(file)\n",
    "        #         writer.writerow([inner_site_path, len(trackers_requests_reject)])\n",
    "        #         file.flush() # bugfix where rows weren't writing: flush() clears internal buffer\n",
    "\n",
    "        # else:\n",
    "        #     with open(reject_file, mode=\"w\", newline=\"\") as file:\n",
    "        #         writer = csv.writer(file)\n",
    "        #         writer.writerow([\"Inner Site Path\", \"Length of Detected List\"])\n",
    "        #         writer.writerow([inner_site_path, len(trackers_requests_reject)])\n",
    "        #         file.flush()\n",
    "\n",
    "\n",
    "# Create DataFrames for detected trackers in normal and after_reject crawls\n",
    "# Each tracker is in a row, with its domain and inner site path\n",
    "df_normal = pd.DataFrame(detected_trackers_from_requests_normal)\n",
    "df_after_reject = pd.DataFrame(detected_trackers_from_requests_reject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_normal.info()\n",
    "# df_after_reject.info()\n",
    "# df_normal.head(15)\n",
    "# df_after_reject.head(15)\n",
    "# df_normal.to_csv(\"analysis/depth1_normal_1.csv\")\n",
    "# df_after_reject.to_csv(\"analysis/depth1_after_reject_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Trackers that Remain After Rejecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates in df_normal (if every value in a row is the same, it is considered a duplicate)\n",
    "df_normal_unique = df_normal.drop_duplicates()\n",
    "\n",
    "# Perform an inner merge (cookies in df_after_reject are kept if they are in df_normal_unique)\n",
    "merged_df = df_after_reject.merge(df_normal_unique, on=[\"Domain\", \"Inner Site Path\", \"Cookie Name\", \"Cookie Value\"], how=\"inner\")\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by domain and set Num_Inner_Pages using the dictionary domains_paths_normal\n",
    "df_normal_domains = df_normal.groupby('Domain', as_index=False).agg(\n",
    "    Num_Inner_Pages=('Domain', lambda x: len(domains_paths_normal.get(x.iloc[0], []))), # Use the length of inner site paths in the domains_paths_normal dictionary\n",
    "    Num_Trackers_Per_Domain=('Cookie Name', 'count')  # Count the number of trackers for each domain\n",
    ")\n",
    "\n",
    "df_normal_domains[\"Average Trackers Per Page\"] = df_normal_domains[\"Num_Trackers_Per_Domain\"] / df_normal_domains[\"Num_Inner_Pages\"]\n",
    "\n",
    "\n",
    "# Group by domain and set Num_Inner_Pages using the dictionary domains_paths_reject\n",
    "df_after_reject_domains = df_after_reject.groupby('Domain', as_index=False).agg(\n",
    "    Num_Inner_Pages=('Domain', lambda x: len(domains_paths_reject.get(x.iloc[0], []))), # Use the length of inner site paths in the domains_paths_reject dictionary\n",
    "    Num_Trackers_Per_Domain=('Cookie Name', 'count')  # Count the number of trackers for each domain\n",
    ")\n",
    "\n",
    "df_after_reject_domains[\"Average Trackers Per Page\"] = df_after_reject_domains[\"Num_Trackers_Per_Domain\"] / df_after_reject_domains[\"Num_Inner_Pages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_normal.info()\n",
    "# df_normal_domains.info()\n",
    "df_normal_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_after_reject_domains.info()\n",
    "df_after_reject_domains\n",
    "# df_after_reject.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Trackers Kept After Rejecting, Grouped by Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by \"Domain\" and count the number of trackers for each domain\n",
    "domain_counts = merged_df.groupby(\"Domain\").size().reset_index(name=\"Tracker Count\")\n",
    "\n",
    "# Sort the DataFrame by descending \"Tracker Count\"\n",
    "domain_counts_sorted = domain_counts.sort_values(by=\"Tracker Count\", ascending=False)\n",
    "# domain_counts\n",
    "domain_counts_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count domains where tracking cookies sent in both \"Normal\" and \"After Reject\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for (_, normal_row) in df_normal_domains.iterrows():\n",
    "    reject_row = df_after_reject_domains.loc[df_after_reject_domains['Domain'] == normal_row['Domain']].squeeze(axis=0)\n",
    "    if reject_row.empty:\n",
    "        reject_row.Num_Trackers_Per_Domain = 0\n",
    "\n",
    "    if normal_row.Num_Trackers_Per_Domain > 0 or reject_row.Num_Trackers_Per_Domain > 0:\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Tracking Cookies Across All Inner Pages (Regardless of Domain)\n",
    "Run this cell to check that number of complete+incomplete pages equals total inner pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_trackers(reject_filepath, normal_filepath):\n",
    "    no_trackers_after_reject = []  # List of inner site paths with trackers in normal crawl, but no trackers after rejection\n",
    "    increased_trackers = []  # List of inner site paths with more trackers after rejection than in normal crawl\n",
    "    never_trackers = []  # List of inner site paths with no trackers in either normal or rejection crawl\n",
    "    violating_sites = []  # List of inner site paths with trackers after we click the reject button\n",
    "\n",
    "    with open(reject_filepath, 'r') as reject_file, open(normal_filepath, 'r') as normal_file:\n",
    "        read_reject = csv.reader(reject_file)\n",
    "        read_normal = csv.reader(normal_file)\n",
    "\n",
    "        # Skip header\n",
    "        next(read_reject)\n",
    "        next(read_normal)\n",
    "\n",
    "        length = 0\n",
    "\n",
    "        # Since both csvs are sorted by inner site path, we can just iterate through both at the same time\n",
    "        for normal, after_reject in zip(read_normal, read_reject):\n",
    "            inner_site_path, num_trackers_normal = normal\n",
    "            _, num_trackers_reject = after_reject\n",
    "\n",
    "            if inner_site_path != _:\n",
    "                raise RuntimeError(\"Inner site paths do not match\")\n",
    "\n",
    "            num_trackers_normal = int(num_trackers_normal)\n",
    "            num_trackers_reject = int(num_trackers_reject)\n",
    "\n",
    "            if num_trackers_normal > 0 and num_trackers_reject == 0:  # if there are trackers in normal crawl, but not after reject\n",
    "                no_trackers_after_reject.append(inner_site_path)\n",
    "\n",
    "            if num_trackers_normal < num_trackers_reject:  # if there are more trackers after reject than in normal crawl\n",
    "                increased_trackers.append(inner_site_path)\n",
    "\n",
    "            if num_trackers_normal == 0 and num_trackers_reject == 0:  # if there are no trackers in either normal or reject\n",
    "                never_trackers.append(inner_site_path)\n",
    "\n",
    "            if num_trackers_reject != 0:  # if there are trackers in reject\n",
    "                violating_sites.append(inner_site_path)\n",
    "\n",
    "            length += 1\n",
    "\n",
    "    # from previous cell\n",
    "    print(\"Total inner pages:\", total_inner_pages)\n",
    "    print(\"Incomplete inner pages:\", incomplete_runs)\n",
    "    \n",
    "    print(\"Complete inner pages:\", length)\n",
    "    print(\"Inner pages that removed all trackers after rejection:\", len(no_trackers_after_reject))\n",
    "    print(\"Inner pages with increased trackers after rejection:\", len(increased_trackers))\n",
    "    print(\"Inner pages that never contained trackers:\", len(never_trackers))\n",
    "    print(\"Inner pages that sent cookies to 3rd party trackers after rejection:\", len(violating_sites))\n",
    "\n",
    "\n",
    "def get_length_detected_list(csv_reader, inner_site_path):\n",
    "    for row in csv_reader:\n",
    "        current_inner_site_path, length_detected_list = row\n",
    "        if current_inner_site_path == inner_site_path:\n",
    "            return length_detected_list\n",
    "\n",
    "    return '0'  # If inner_site_path not found, return '0'\n",
    "\n",
    "\n",
    "compare_trackers('analysis/depth0_after_reject.csv', 'analysis/depth0_normal.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cookie-classify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
